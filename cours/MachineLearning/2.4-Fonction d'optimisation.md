
# <span class="h1">Fonction d'optimisation</span>

#cours #MachineLearning 


> [!summary] 
> *En machine learning, les fonctions d'optimisation sont des algorithmes utilisÃ©s pour ajuster les paramÃ¨tres des modÃ¨les de maniÃ¨re Ã  minimiser la fonction de coÃ»t (ou de perte). *

## <span class="h2">La Descente de Gradient</span>

> [!info] 
> *La descente de gradient est une technique d'optimisation largement utilisÃ©e dans le machine learning et le deep learning pour ajuster les paramÃ¨tres d'un modÃ¨le en minimisant une fonction de coÃ»t. Voici un aperÃ§u dÃ©taillÃ© de son fonctionnement, de ses variantes, et de son application. *

### <span class="h3">Principe de Base</span>

La descente de gradient cherche Ã  trouver les valeurs optimales des paramÃ¨tres $\theta$ d'un modÃ¨le en minimisant la fonction de coÃ»t $j(\theta)$. Cette fonction de coÃ»t mesure l'erreur entre les prÃ©dictions du modÃ¨le et les valeurs rÃ©elles des donnÃ©es d'entraÃ®nement.

La mise Ã  jour des paramÃ¨tres $\theta$ se fait selon la rÃ¨gle suivante :

$$\theta â† (\theta âˆ’ \eta \nabla ğ½(\theta))$$

- $\theta$ reprÃ©sente les paramÃ¨tres du modÃ¨le.
- $\eta$ est le taux d'apprentissage (learning rate), un hyperparamÃ¨tre qui dÃ©termine la taille des pas de mise Ã  jour.
- $\nabla ğ½(\theta)$ est le gradient de la fonction de coÃ»t par rapport aux paramÃ¨tres $\theta$.

Le gradient $\nabla ğ½(\theta)$ est un vecteur de dÃ©rivÃ©es partielles de la fonction de coÃ»t, indiquant la direction et la pente de la plus grande augmentation de la fonction. En prenant le pas dans la direction opposÃ©e au gradient, on se dÃ©place vers un minimum de la fonction de coÃ»t.

![[gradient | 350]]


### <span class="h3">Processus de la Descente de Gradient</span>

- Initialisation des ParamÃ¨tres : Commencer avec des valeurs initiales pour les paramÃ¨tres $\theta$ (souvent alÃ©atoires ou nulles).
  
- Calcul du Gradient : Pour chaque itÃ©ration, calculer le gradient de la fonction de coÃ»t par rapport aux paramÃ¨tres actuels $\theta$. 
  $$\nablağ½(ğœƒ)=\partial ğ½(\partial \theta_1) \dots \partial ğ½(\partial \theta_n)$$
    
- Mise Ã  Jour des ParamÃ¨tres : Mettre Ã  jour les paramÃ¨tres en suivant la rÃ¨gle de mise Ã  jour.
  $$\theta â† (\theta âˆ’ \eta \nabla ğ½(\theta))$$

- VÃ©rification de la Convergence : RÃ©pÃ©ter le processus jusqu'Ã  ce que la convergence soit atteinte, c'est-Ã -dire jusqu'Ã  ce que les changements de la fonction de coÃ»t soient infimes ou que le nombre maximum d'itÃ©rations soit atteint.

### <span class="h3">Avantages et InconvÃ©nients</span>

**Avantages** :

- SimplicitÃ© et facilitÃ© de mise en Å“uvre.
- Efficace pour de nombreux problÃ¨mes d'optimisation.

**InconvÃ©nients** :

- Sensible au choix du taux d'apprentissage ğœ‚Î·.
- Peut converger lentement pour des fonctions de coÃ»t complexes.
- Peut rester coincÃ© dans des minima locaux pour des fonctions non convexes.

![[gradient2 | 350]]





### <span class="h3">Variantes de la Descente de Gradient</span>


#### <span class="h4">Batch Gradient Descent</span>

> [!info] 
> Cette mÃ©thode utilise l'ensemble complet des donnÃ©es d'entraÃ®nement pour calculer le gradient Ã  chaque itÃ©ration. 

 A dÃ©finir


#### <span class="h4">Stochastic Gradient Descent (SGD)</span>

> [!info] 
> La descente de gradient stochastique (SGD) est une variation de la descente de gradient oÃ¹ les mises Ã  jour des paramÃ¨tres sont effectuÃ©es pour chaque exemple de donnÃ©es plutÃ´t que pour l'ensemble complet des donnÃ©es. 

ğœƒâ†ğœƒâˆ’ğœ‚âˆ‡ğ½(ğœƒ;ğ‘¥(ğ‘–),ğ‘¦(ğ‘–))

OÃ¹ (ğ‘¥(ğ‘–),ğ‘¦(ğ‘–))(x(i),y(i)) est un exemple de donnÃ©es d'entraÃ®nement.

**Avantages** :

- Convergence plus rapide pour de grands ensembles de donnÃ©es.
- Peut Ã©chapper aux minima locaux grÃ¢ce aux fluctuations dans les mises Ã  jour.

**InconvÃ©nients** :

- Oscillations autour du minimum global.
- Moins stable que le batch gradient descent.


### <span class="h4">Descente de Gradient avec Moment</span>

> [!info] 
> L'optimisation avec momentum accumule un terme de moment des gradients passÃ©s pour accÃ©lÃ©rer les mises Ã  jour et rÃ©duire les oscillations. 

**Formule** : ğ‘£ğ‘¡=ğ›½ğ‘£ğ‘¡âˆ’1+(1âˆ’ğ›½)âˆ‡ğ½(ğœƒğ‘¡)vtâ€‹=Î²vtâˆ’1â€‹+(1âˆ’Î²)âˆ‡J(Î¸tâ€‹) ğœƒâ†ğœƒâˆ’ğœ‚ğ‘£ğ‘¡Î¸â†Î¸âˆ’Î·vtâ€‹

OÃ¹ :

- ğ‘£ğ‘¡vtâ€‹ est la vitesse (momentum) accumulÃ©e jusqu'Ã  l'itÃ©ration ğ‘¡t.
- ğ›½Î² est le coefficient de moment (typiquement proche de 1).


#### <span class="h4">AdaGrad (Adaptive Gradient Algorithm)</span>

> [!info] 
> AdaGrad adapte le taux d'apprentissage pour chaque paramÃ¨tre en fonction des gradients passÃ©s. Les paramÃ¨tres avec de grands gradients reÃ§oivent des taux d'apprentissage plus petits. 

**Formule** : ğœƒğ‘–â†ğœƒğ‘–âˆ’ğœ‚ğºğ‘–ğ‘–+ğœ–âˆ‡ğ½(ğœƒğ‘–)Î¸iâ€‹â†Î¸iâ€‹âˆ’Giiâ€‹+Ïµâ€‹Î·â€‹âˆ‡J(Î¸iâ€‹)

OÃ¹ :

- ğºG est la somme des carrÃ©s des gradients passÃ©s.
- ğœ–Ïµ est un petit terme de stabilisation pour Ã©viter la division par zÃ©ro.






## <span class="h2">RMSProp (Root Mean Square Propagation)</span>

RMSProp est une amÃ©lioration d'AdaGrad qui divise le taux d'apprentissage par la moyenne mobile des gradients au carrÃ©, plutÃ´t que par la somme de tous les gradients passÃ©s.

**Formule** : ğ¸[ğ‘”2]ğ‘¡=ğ›½ğ¸[ğ‘”2]ğ‘¡âˆ’1+(1âˆ’ğ›½)ğ‘”ğ‘¡2E[g2]tâ€‹=Î²E[g2]tâˆ’1â€‹+(1âˆ’Î²)gt2â€‹ ğœƒâ†ğœƒâˆ’ğœ‚ğ¸[ğ‘”2]ğ‘¡+ğœ–ğ‘”ğ‘¡Î¸â†Î¸âˆ’E[g2]tâ€‹+Ïµâ€‹Î·â€‹gtâ€‹

OÃ¹ :

- ğ¸[ğ‘”2]ğ‘¡E[g2]tâ€‹ est la moyenne mobile exponentielle des carrÃ©s des gradients.
- ğ‘”ğ‘¡gtâ€‹ est le gradient Ã  l'itÃ©ration ğ‘¡t.

#### 6. Adam (Adaptive Moment Estimation)

Adam combine les avantages d'AdaGrad et de RMSProp en utilisant Ã  la fois le moment des gradients et une moyenne mobile des carrÃ©s des gradients.

**Formule** : ğ‘šğ‘¡=ğ›½1ğ‘šğ‘¡âˆ’1+(1âˆ’ğ›½1)ğ‘”ğ‘¡mtâ€‹=Î²1â€‹mtâˆ’1â€‹+(1âˆ’Î²1â€‹)gtâ€‹ ğ‘£ğ‘¡=ğ›½2ğ‘£ğ‘¡âˆ’1+(1âˆ’ğ›½2)ğ‘”ğ‘¡2vtâ€‹=Î²2â€‹vtâˆ’1â€‹+(1âˆ’Î²2â€‹)gt2â€‹ ğ‘š^ğ‘¡=ğ‘šğ‘¡1âˆ’ğ›½1ğ‘¡m^tâ€‹=1âˆ’Î²1tâ€‹mtâ€‹â€‹ ğ‘£^ğ‘¡=ğ‘£ğ‘¡1âˆ’ğ›½2ğ‘¡v^tâ€‹=1âˆ’Î²2tâ€‹vtâ€‹â€‹ ğœƒâ†ğœƒâˆ’ğœ‚ğ‘š^ğ‘¡ğ‘£^ğ‘¡+ğœ–Î¸â†Î¸âˆ’v^tâ€‹â€‹+ÏµÎ·m^tâ€‹â€‹

OÃ¹ :

- ğ‘šğ‘¡mtâ€‹ et ğ‘£ğ‘¡vtâ€‹ sont les estimations de premier et second moment.
- ğ›½1Î²1â€‹ et ğ›½2Î²2â€‹ sont des hyperparamÃ¨tres typiquement choisis comme ğ›½1=0.9Î²1â€‹=0.9 et ğ›½2=0.999Î²2â€‹=0.999.