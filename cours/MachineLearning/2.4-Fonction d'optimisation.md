
# <span class="h1">Fonction d'optimisation</span>

#cours #MachineLearning 


> [!summary] 
> *En machine learning, les fonctions d'optimisation sont des algorithmes utilisés pour ajuster les paramètres des modèles de manière à minimiser la fonction de coût (ou de perte). *

## <span class="h2">La Descente de Gradient</span>

> [!info] 
> *La descente de gradient est une technique d'optimisation largement utilisée dans le machine learning et le deep learning pour ajuster les paramètres d'un modèle en minimisant une fonction de coût. Voici un aperçu détaillé de son fonctionnement, de ses variantes, et de son application. *

### <span class="h3">Principe de Base</span>

La descente de gradient cherche à trouver les valeurs optimales des paramètres $\theta$ d'un modèle en minimisant la fonction de coût $j(\theta)$. Cette fonction de coût mesure l'erreur entre les prédictions du modèle et les valeurs réelles des données d'entraînement.

La mise à jour des paramètres $\theta$ se fait selon la règle suivante :

$$\theta ← (\theta − \eta \nabla 𝐽(\theta))$$

- $\theta$ représente les paramètres du modèle.
- $\eta$ est le taux d'apprentissage (learning rate), un hyperparamètre qui détermine la taille des pas de mise à jour.
- $\nabla 𝐽(\theta)$ est le gradient de la fonction de coût par rapport aux paramètres $\theta$.

Le gradient $\nabla 𝐽(\theta)$ est un vecteur de dérivées partielles de la fonction de coût, indiquant la direction et la pente de la plus grande augmentation de la fonction. En prenant le pas dans la direction opposée au gradient, on se déplace vers un minimum de la fonction de coût.

![[gradient | 350]]


### <span class="h3">Processus de la Descente de Gradient</span>

- Initialisation des Paramètres : Commencer avec des valeurs initiales pour les paramètres $\theta$ (souvent aléatoires ou nulles).
  
- Calcul du Gradient : Pour chaque itération, calculer le gradient de la fonction de coût par rapport aux paramètres actuels $\theta$. 
  $$\nabla𝐽(𝜃)=\partial 𝐽(\partial \theta_1) \dots \partial 𝐽(\partial \theta_n)$$
    
- Mise à Jour des Paramètres : Mettre à jour les paramètres en suivant la règle de mise à jour.
  $$\theta ← (\theta − \eta \nabla 𝐽(\theta))$$

- Vérification de la Convergence : Répéter le processus jusqu'à ce que la convergence soit atteinte, c'est-à-dire jusqu'à ce que les changements de la fonction de coût soient infimes ou que le nombre maximum d'itérations soit atteint.

### <span class="h3">Avantages et Inconvénients</span>

**Avantages** :

- Simplicité et facilité de mise en œuvre.
- Efficace pour de nombreux problèmes d'optimisation.

**Inconvénients** :

- Sensible au choix du taux d'apprentissage 𝜂η.
- Peut converger lentement pour des fonctions de coût complexes.
- Peut rester coincé dans des minima locaux pour des fonctions non convexes.

![[gradient2 | 350]]





### <span class="h3">Variantes de la Descente de Gradient</span>


#### <span class="h4">Batch Gradient Descent</span>

> [!info] 
> Cette méthode utilise l'ensemble complet des données d'entraînement pour calculer le gradient à chaque itération. 

 A définir


#### <span class="h4">Stochastic Gradient Descent (SGD)</span>

> [!info] 
> La descente de gradient stochastique (SGD) est une variation de la descente de gradient où les mises à jour des paramètres sont effectuées pour chaque exemple de données plutôt que pour l'ensemble complet des données. 

𝜃←𝜃−𝜂∇𝐽(𝜃;𝑥(𝑖),𝑦(𝑖))

Où (𝑥(𝑖),𝑦(𝑖))(x(i),y(i)) est un exemple de données d'entraînement.

**Avantages** :

- Convergence plus rapide pour de grands ensembles de données.
- Peut échapper aux minima locaux grâce aux fluctuations dans les mises à jour.

**Inconvénients** :

- Oscillations autour du minimum global.
- Moins stable que le batch gradient descent.


### <span class="h4">Descente de Gradient avec Moment</span>

> [!info] 
> L'optimisation avec momentum accumule un terme de moment des gradients passés pour accélérer les mises à jour et réduire les oscillations. 

**Formule** : 𝑣𝑡=𝛽𝑣𝑡−1+(1−𝛽)∇𝐽(𝜃𝑡)vt​=βvt−1​+(1−β)∇J(θt​) 𝜃←𝜃−𝜂𝑣𝑡θ←θ−ηvt​

Où :

- 𝑣𝑡vt​ est la vitesse (momentum) accumulée jusqu'à l'itération 𝑡t.
- 𝛽β est le coefficient de moment (typiquement proche de 1).


#### <span class="h4">AdaGrad (Adaptive Gradient Algorithm)</span>

> [!info] 
> AdaGrad adapte le taux d'apprentissage pour chaque paramètre en fonction des gradients passés. Les paramètres avec de grands gradients reçoivent des taux d'apprentissage plus petits. 

**Formule** : 𝜃𝑖←𝜃𝑖−𝜂𝐺𝑖𝑖+𝜖∇𝐽(𝜃𝑖)θi​←θi​−Gii​+ϵ​η​∇J(θi​)

Où :

- 𝐺G est la somme des carrés des gradients passés.
- 𝜖ϵ est un petit terme de stabilisation pour éviter la division par zéro.






## <span class="h2">RMSProp (Root Mean Square Propagation)</span>

RMSProp est une amélioration d'AdaGrad qui divise le taux d'apprentissage par la moyenne mobile des gradients au carré, plutôt que par la somme de tous les gradients passés.

**Formule** : 𝐸[𝑔2]𝑡=𝛽𝐸[𝑔2]𝑡−1+(1−𝛽)𝑔𝑡2E[g2]t​=βE[g2]t−1​+(1−β)gt2​ 𝜃←𝜃−𝜂𝐸[𝑔2]𝑡+𝜖𝑔𝑡θ←θ−E[g2]t​+ϵ​η​gt​

Où :

- 𝐸[𝑔2]𝑡E[g2]t​ est la moyenne mobile exponentielle des carrés des gradients.
- 𝑔𝑡gt​ est le gradient à l'itération 𝑡t.

#### 6. Adam (Adaptive Moment Estimation)

Adam combine les avantages d'AdaGrad et de RMSProp en utilisant à la fois le moment des gradients et une moyenne mobile des carrés des gradients.

**Formule** : 𝑚𝑡=𝛽1𝑚𝑡−1+(1−𝛽1)𝑔𝑡mt​=β1​mt−1​+(1−β1​)gt​ 𝑣𝑡=𝛽2𝑣𝑡−1+(1−𝛽2)𝑔𝑡2vt​=β2​vt−1​+(1−β2​)gt2​ 𝑚^𝑡=𝑚𝑡1−𝛽1𝑡m^t​=1−β1t​mt​​ 𝑣^𝑡=𝑣𝑡1−𝛽2𝑡v^t​=1−β2t​vt​​ 𝜃←𝜃−𝜂𝑚^𝑡𝑣^𝑡+𝜖θ←θ−v^t​​+ϵηm^t​​

Où :

- 𝑚𝑡mt​ et 𝑣𝑡vt​ sont les estimations de premier et second moment.
- 𝛽1β1​ et 𝛽2β2​ sont des hyperparamètres typiquement choisis comme 𝛽1=0.9β1​=0.9 et 𝛽2=0.999β2​=0.999.