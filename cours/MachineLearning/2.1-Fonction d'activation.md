
## <span class="h2">Fonction d'Activation </span>

#cours #MachineLearning 

> [!summary] 
> Les fonctions d'activation sont essentielles dans les réseaux de neurones car elles introduisent de la non linéarité dans le modèle, permettant ainsi de capturer des relations complexes entre les entrées et les sorties. Cela ce traduit par la transformation des sommes pondérées des entrées pour produire la sortie. Dans les modèles linéaires simples, il n'y a pas toujours une fonction d'activation explicite. 


#### <span class="h4">Fonction d'Activation Linéaire (Identité)</span>

La fonction d'activation linéaire est simplement l'identité, ce qui signifie que la sortie est égale à l'entrée. Elle est utilisée dans les modèles linéaires classiques.

$$𝑓(𝑥)=𝑥$$
<u>Caractéristiques :</u> 

- Simplicité, adaptée aux problèmes de régression.
- Ne capture pas la non-linéarité.



### <span class="h4">Fonction Sigmoïde (Sigmoid)</span>

La fonction sigmoïde transforme les valeurs en une sortie comprise entre 0 et 1. Elle est souvent utilisée pour les modèles de classification binaire.
 $$f(x)=\frac{1}{1+e^{−x}}​​$$
<u>Caractéristiques :</u>

- Sortie comprise entre 0 et 1.
- Utilisée pour la classification binaire.
- Problème de saturation : les gradients deviennent très faibles pour les valeurs extrêmes de 𝑥x, ce qui ralentit l'apprentissage.




#### <span class="h4">Fonction Tangente Hyperbolique (Tanh)</span>

La fonction tanh est similaire à la sigmoïde mais elle transforme les valeurs en une sortie comprise entre -1 et 1.

$$f(x)=tanh(x)= \frac{e^x+e^{−x}}{e^x−e^{−x}}​$$
<u>Caractéristiques :</u>

- Sortie comprise entre -1 et 1.
- Centrée autour de 0, ce qui peut aider à la convergence plus rapide que la sigmoïde.
- Problème de saturation similaire à la sigmoïde.

<u>Exemple d'application :</u> Modèles de classification où les sorties peuvent être négatives ou positives.





#### <span class="h4">Unité Linéaire Rectifiée (ReLU)</span>

La fonction ReLU est largement utilisée dans les réseaux de neurones profonds. Elle renvoie 0 pour les entrées négatives et la valeur d'entrée elle-même pour les entrées positives.

$$f(x)=max(0,x)$$

<u>Caractéristiques :</u>

- Sortie non bornée pour les valeurs positives et 0 pour les valeurs négatives.
- Introduction de la non-linéarité sans problème de saturation pour les valeurs positives.
- Peut entraîner des neurones "morts" (sortie toujours 0) si beaucoup de valeurs négatives.

<u>Exemple d'application :</u> Réseaux de neurones convolutifs pour la reconnaissance d'images.
