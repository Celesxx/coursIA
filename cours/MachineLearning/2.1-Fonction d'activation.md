
## <span class="h2">Fonction d'Activation </span>

#cours #MachineLearning 

> [!summary] 
> Les fonctions d'activation sont essentielles dans les rÃ©seaux de neurones car elles introduisent de la non linÃ©aritÃ© dans le modÃ¨le, permettant ainsi de capturer des relations complexes entre les entrÃ©es et les sorties. Cela ce traduit par la transformation des sommes pondÃ©rÃ©es des entrÃ©es pour produire la sortie. Dans les modÃ¨les linÃ©aires simples, il n'y a pas toujours une fonction d'activation explicite. 


#### <span class="h4">Fonction d'Activation LinÃ©aire (IdentitÃ©)</span>

La fonction d'activation linÃ©aire est simplement l'identitÃ©, ce qui signifie que la sortie est Ã©gale Ã  l'entrÃ©e. Elle est utilisÃ©e dans les modÃ¨les linÃ©aires classiques.

$$ğ‘“(ğ‘¥)=ğ‘¥$$
<u>CaractÃ©ristiques :</u> 

- SimplicitÃ©, adaptÃ©e aux problÃ¨mes de rÃ©gression.
- Ne capture pas la non-linÃ©aritÃ©.



### <span class="h4">Fonction SigmoÃ¯de (Sigmoid)</span>

La fonction sigmoÃ¯de transforme les valeurs en une sortie comprise entre 0 et 1. Elle est souvent utilisÃ©e pour les modÃ¨les de classification binaire.
 $$f(x)=\frac{1}{1+e^{âˆ’x}}â€‹â€‹$$
<u>CaractÃ©ristiques :</u>

- Sortie comprise entre 0 et 1.
- UtilisÃ©e pour la classification binaire.
- ProblÃ¨me de saturation : les gradients deviennent trÃ¨s faibles pour les valeurs extrÃªmes de ğ‘¥x, ce qui ralentit l'apprentissage.




#### <span class="h4">Fonction Tangente Hyperbolique (Tanh)</span>

La fonction tanh est similaire Ã  la sigmoÃ¯de mais elle transforme les valeurs en une sortie comprise entre -1 et 1.

$$f(x)=tanh(x)= \frac{e^x+e^{âˆ’x}}{e^xâˆ’e^{âˆ’x}}â€‹$$
<u>CaractÃ©ristiques :</u>

- Sortie comprise entre -1 et 1.
- CentrÃ©e autour de 0, ce qui peut aider Ã  la convergence plus rapide que la sigmoÃ¯de.
- ProblÃ¨me de saturation similaire Ã  la sigmoÃ¯de.

<u>Exemple d'application :</u> ModÃ¨les de classification oÃ¹ les sorties peuvent Ãªtre nÃ©gatives ou positives.





#### <span class="h4">UnitÃ© LinÃ©aire RectifiÃ©e (ReLU)</span>

La fonction ReLU est largement utilisÃ©e dans les rÃ©seaux de neurones profonds. Elle renvoie 0 pour les entrÃ©es nÃ©gatives et la valeur d'entrÃ©e elle-mÃªme pour les entrÃ©es positives.

$$f(x)=max(0,x)$$

<u>CaractÃ©ristiques :</u>

- Sortie non bornÃ©e pour les valeurs positives et 0 pour les valeurs nÃ©gatives.
- Introduction de la non-linÃ©aritÃ© sans problÃ¨me de saturation pour les valeurs positives.
- Peut entraÃ®ner des neurones "morts" (sortie toujours 0) si beaucoup de valeurs nÃ©gatives.

<u>Exemple d'application :</u> RÃ©seaux de neurones convolutifs pour la reconnaissance d'images.
